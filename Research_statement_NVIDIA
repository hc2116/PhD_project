
This research research project falls in the category of cyber-security and network intrusion detection. The threat of large enterprise network breaches has been growing significantly in the last years, which is in part due to the rise of custom-built targeted malware, and stealthy attack strategies, where malicious activity masked as benign one circumvents firewalls and rule-based detection systems, sometimes using adversarial learning.

Commercial signature-based detection methods are dependent on databases of known attack signatures and malicious behaviour characteristics, and are therefore often defenseless against zero-day malware and evasive tactics. In order to improve robustness against these two strategies, we want to develop anomaly-based methods to detect any malicious behaviour as anomalous network activity that deviates from an accurate previously learned model of normal network activity. 


This particular project aims to learn model with semantic representations of network packet sequences characteristics in TCP connections. This model should be able to predict characteristics (direction, size, inter arrival time, flags) of individual network packets from the context of the other packets appearing in a sequence. Such a model should be learned either for individual machines, or for a number of similar machines in a network. That way, the model will learn the different communication patterns of software installed on a machine, and identify new and anomalous patterns as large prediction errors, indicating the presence of a malicious software or actor. Initially, we want to focus on learning representations of the first few packets in a network connection, as this is the negotiation phase where important access credentials etc. are transmitted. Several types of attacks exploit loopholes accessible at this negotiation stage, such as buffer overflows, XSS, or data-manipulation attacks. Extension to other stages in the connection would be straightforward.

To learn such representations, we want to use LSTM-autoencoders applied to sequences of fixed length. LSTM-autoencoders are a deep-learning framework that can generalize input patterns via information compression, and thus build a representation model of a dataset in an unsupervised way. For that, the input information of a sequence is processed sequentially, and then pushed through an information bottleneck, allowing only the most important information to pass through. From that, the model tries to recreate the input information as accurately as possible. By constricting the passed information, the model is forced to learn contextual patterns about several events. Consequently, an input sequence that does not conform with the contextual patterns will not be recreated accurately, indicating anomalous behaviour.

Parameters in deep-learning methods are often trained using stochastic gradient descent in mini-batches, and many modelling libraries such as Tensorflow or PyTorch allow significant accelerations of this learning process via GPU-processing. We therefore would benefit from access to a strong GPU as we could train on significantly larger datasets, therefore enabling more accurate models that have more information about rare events. We will likely be using PyTorch to implement our model.


We had previous success in using contextual/semantic modelling for intrusion detection. Chen, Aspinall, Gordon, Sutton, and Muttik ("More Semantics More Robust: Improving Android Malware Classifiers", 2016) recently demonstrated the effectiveness of state-based software models in the identification of malicious sequences actions in a stream of permission and API calls of Android applications.  The usage of “semantic features” allowed the identification of different reoccurring sequential patterns, which was then used for discovery of new malware instances via semantic similarity to known instances. This improved the overall robustness of the classifier drastically. 

We have furthermore explored the application of LSTM networks to network events for the discovery of sequential patterns, and are currently in the progress of publishing our results. In this approach ("Contextual anomaly detection using LSTM-networks", Clausen, Sabate, Grov, Chen, Aspinall), we trained an LSTM network on short sequences of network flows to predict the probability of the protocol, direction, and network port of each flow conditional on previous flows. This approach improves the detection rates of low-volume R2L and botnet activities.


The described project will be primarily conducted by my PhD-student Henry Clausen, who is currently in his first year.




Research Statement

Please review the GPU Grant Program website for full program details before submitting this form. All current updates to the program are posted there first. Professors, Researchers and Advisors should complete this online form to request a GPU for research purposes for themselves and/or their teams.In order to review your request, we require a statement of research summary and CV. The "Additional Materials" field should NOT be used to attach a full proposal in lieu of a statement of research.

Your statement of research must include the following:
1) short summary/description of the research project
2) how you and/or your team will use the GPU
3) list of most recent publications {if you have any}

GPU Grant Requests are typically reviewed every two weeks with notification taking place as soon as the review is final.There is a limit of (1) GPU grant per person, per year, per project. If caught abusing the limits, you will risk being banned from the program. We currently offer Jetson TX2 Developer Kit and GeForce Titan V worldwide. We are not offering Quadro or Tesla products at this time. We offer DRIVE PX 2 AutoChauffeur Development Platform in the US and Canada only via the DRIVE Platform Grant Program. As a general rule, we do not provide feedback on submission.

Students, including PhD candidates, should not submit GPU Grant Requests for themselves or on behalf of their professors. All submissions must come from a Professor, fully paid Academic Researcher or Advisor. If you have multiple students interested in using GPUs for their research, one submission should be made for your group keeping in mind the overall goals for the next year. Sharing resources is encouraged.

All information should match the professor, researcher and/or advisor information and the log in/profile email should be the same as the email listed in the form. Any misinformation or applications submitted by students for their professors will result in automatic decline.

NVIDIA Academic GPU grants are "seeding" gifts of (1) GPU intended to enable researchers to begin a new project and/or gain the preliminary results to support a larger proposal to other funding agencies. Our goal is "breadth-first" seeding and grants are for (1) GPU. We do not donate workstations or servers, large quantities of GPUs or server rack mountable GPUs. If you request more than one GPU for your research or your team, be sure to clearly explain why multiple GPUs are needed in your statement of research. We rarely award more than one GPU.

Please provide an accurate and complete FedEx/DHL acceptable shipping address and phone number in the form below. NO PO BOXES. The address provided will be EXACTLY where the grants are shipped. Street Addresses should NOT include any information included in other fields (eg. do not put university, city, state zip on any of the street address lines). We pull all relevant fields into the shipping address automatically. We spend hours fixing your mistakes, not following these instructions causes a lot of delays for everyone. If this information is incomplete or incorrect in any way, the submission or award could be rescinded or delayed - double check for accuracy before submitting.

If you have any questions, please feel free to email us at universitypartnership@nvidia.com. Once you submit this form, you will NOT be able to edit. We also delete draft submissions on a monthly basis.
